{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437429c9-fee3-4b03-967f-bc16d1d06150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "from time import time\n",
    "\n",
    "# Load MNIST and FashionMNIST datasets\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "(X_train_fashion, y_train_fashion), (X_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "X_train_mnist = X_train_mnist / 255.0\n",
    "X_test_mnist = X_test_mnist / 255.0\n",
    "X_train_fashion = X_train_fashion / 255.0\n",
    "X_test_fashion = X_test_fashion / 255.0\n",
    "\n",
    "# Define classical and quantum approaches with some mock functions (replace with real models)\n",
    "def classical_model(train_data, test_data):\n",
    "    start_time = time()\n",
    "    # Simulate training and evaluation of a classical model\n",
    "    train_loss = np.random.uniform(0.04, 0.08, len(train_data))\n",
    "    test_accuracy = np.random.uniform(85, 90, len(test_data))\n",
    "    end_time = time()\n",
    "    return train_loss, test_accuracy, end_time - start_time\n",
    "\n",
    "def quantum_model(train_data, test_data):\n",
    "    start_time = time()\n",
    "    # Simulate training and evaluation of a quantum model\n",
    "    train_loss = np.random.uniform(0.03, 0.06, len(train_data))\n",
    "    test_accuracy = np.random.uniform(88, 92, len(test_data))\n",
    "    end_time = time()\n",
    "    return train_loss, test_accuracy, end_time - start_time\n",
    "\n",
    "def hybrid_model(train_data, test_data):\n",
    "    start_time = time()\n",
    "    # Simulate training and evaluation of a hybrid model\n",
    "    train_loss = np.random.uniform(0.02, 0.05, len(train_data))\n",
    "    test_accuracy = np.random.uniform(90, 94, len(test_data))\n",
    "    end_time = time()\n",
    "    return train_loss, test_accuracy, end_time - start_time\n",
    "\n",
    "# Run models on MNIST and FashionMNIST datasets\n",
    "train_loss_classical_mnist, acc_classical_mnist, time_classical_mnist = classical_model(X_train_mnist, X_test_mnist)\n",
    "train_loss_quantum_mnist, acc_quantum_mnist, time_quantum_mnist = quantum_model(X_train_mnist, X_test_mnist)\n",
    "train_loss_hybrid_mnist, acc_hybrid_mnist, time_hybrid_mnist = hybrid_model(X_train_mnist, X_test_mnist)\n",
    "\n",
    "train_loss_classical_fashion, acc_classical_fashion, time_classical_fashion = classical_model(X_train_fashion, X_test_fashion)\n",
    "train_loss_quantum_fashion, acc_quantum_fashion, time_quantum_fashion = quantum_model(X_train_fashion, X_test_fashion)\n",
    "train_loss_hybrid_fashion, acc_hybrid_fashion, time_hybrid_fashion = hybrid_model(X_train_fashion, X_test_fashion)\n",
    "\n",
    "# Plot subplots for comparative analysis with an inset in column 1\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Training Loss (MNIST) with an inset\n",
    "axs[0, 0].plot(train_loss_classical_mnist, label=\"Classical\")\n",
    "axs[0, 0].plot(train_loss_quantum_mnist, label=\"Quantum\")\n",
    "axs[0, 0].plot(train_loss_hybrid_mnist, label=\"Hybrid\")\n",
    "axs[0, 0].set_title('Training Loss (MNIST)')\n",
    "axs[0, 0].set_xlabel('Epochs')\n",
    "axs[0, 0].set_ylabel('Loss')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Add inset to the first subplot\n",
    "inset_ax = fig.add_axes([0.2, 0.7, 0.1, 0.1])\n",
    "inset_ax.plot(train_loss_classical_mnist[:50], label=\"Classical\")\n",
    "inset_ax.plot(train_loss_quantum_mnist[:50], label=\"Quantum\")\n",
    "inset_ax.plot(train_loss_hybrid_mnist[:50], label=\"Hybrid\")\n",
    "inset_ax.set_title(\"Inset: First 50 Epochs\", fontsize=8)\n",
    "inset_ax.legend(fontsize=6)\n",
    "\n",
    "# Subplot 2: Test Accuracy (MNIST)\n",
    "axs[0, 1].plot(acc_classical_mnist, label=\"Classical\")\n",
    "axs[0, 1].plot(acc_quantum_mnist, label=\"Quantum\")\n",
    "axs[0, 1].plot(acc_hybrid_mnist, label=\"Hybrid\")\n",
    "axs[0, 1].set_title('Test Accuracy (MNIST)')\n",
    "axs[0, 1].set_xlabel('Epochs')\n",
    "axs[0, 1].set_ylabel('Accuracy (%)')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Subplot 3: Training Time (MNIST)\n",
    "training_times_mnist = [time_classical_mnist, time_quantum_mnist, time_hybrid_mnist]\n",
    "methods_mnist = ['Classical', 'Quantum', 'Hybrid']\n",
    "axs[0, 2].bar(methods_mnist, training_times_mnist, color=['blue', 'green', 'orange'])\n",
    "axs[0, 2].set_title('Training Time (MNIST)')\n",
    "axs[0, 2].set_xlabel('Methods')\n",
    "axs[0, 2].set_ylabel('Time (seconds)')\n",
    "\n",
    "# Subplot 4: Training Loss (FashionMNIST)\n",
    "axs[1, 0].plot(train_loss_classical_fashion, label=\"Classical\")\n",
    "axs[1, 0].plot(train_loss_quantum_fashion, label=\"Quantum\")\n",
    "axs[1, 0].plot(train_loss_hybrid_fashion, label=\"Hybrid\")\n",
    "axs[1, 0].set_title('Training Loss (FashionMNIST)')\n",
    "axs[1, 0].set_xlabel('Epochs')\n",
    "axs[1, 0].set_ylabel('Loss')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Subplot 5: Test Accuracy (FashionMNIST)\n",
    "axs[1, 1].plot(acc_classical_fashion, label=\"Classical\")\n",
    "axs[1, 1].plot(acc_quantum_fashion, label=\"Quantum\")\n",
    "axs[1, 1].plot(acc_hybrid_fashion, label=\"Hybrid\")\n",
    "axs[1, 1].set_title('Test Accuracy (FashionMNIST)')\n",
    "axs[1, 1].set_xlabel('Epochs')\n",
    "axs[1, 1].set_ylabel('Accuracy (%)')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Subplot 6: Training Time (FashionMNIST)\n",
    "training_times_fashion = [time_classical_fashion, time_quantum_fashion, time_hybrid_fashion]\n",
    "methods_fashion = ['Classical', 'Quantum', 'Hybrid']\n",
    "axs[1, 2].bar(methods_fashion, training_times_fashion, color=['blue', 'green', 'orange'])\n",
    "axs[1, 2].set_title('Training Time (FashionMNIST)')\n",
    "axs[1, 2].set_xlabel('Methods')\n",
    "axs[1, 2].set_ylabel('Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "# Display values from the generated subplots\n",
    "train_loss_classical_mnist[:10], acc_classical_mnist[:10], training_times_mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a941a2-569c-4e6d-b85d-a17b702cbaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "# Load and preprocess datasets\n",
    "def load_datasets():\n",
    "    (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "    (x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()\n",
    "\n",
    "    # Normalize datasets\n",
    "    x_train_mnist, x_test_mnist = x_train_mnist / 255.0, x_test_mnist / 255.0\n",
    "    x_train_fmnist, x_test_fmnist = x_train_fmnist / 255.0, x_test_fmnist / 255.0\n",
    "\n",
    "    return (x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist), (x_train_fmnist, y_train_fmnist, x_test_fmnist, y_test_fmnist)\n",
    "\n",
    "# Build the neural network model\n",
    "def build_model(dropout_rate=0.2):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Function to train the model with specific hyperparameters\n",
    "def train_model(dataset, learning_rate, batch_size, epochs, dropout_rate):\n",
    "    x_train, y_train, x_test, y_test = dataset\n",
    "    model = build_model(dropout_rate)\n",
    "    \n",
    "    # Update learning rate\n",
    "    model.optimizer.learning_rate = learning_rate\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test), verbose=0)\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    return history, test_loss, test_acc\n",
    "\n",
    "# Function to plot training history (accuracy and loss)\n",
    "def plot_history(history, param_info, test_acc, test_loss, subplot_index):\n",
    "    epochs = range(1, len(history.history['accuracy']) + 1)\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 8, subplot_index)  # Subplot 1-4 for accuracy\n",
    "    plt.plot(epochs, history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], label='Val Accuracy')\n",
    "    plt.title(f'Acc\\n{param_info}\\nTest Acc: {test_acc:.4f}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 8, subplot_index + 4)  # Subplot 5-8 for loss\n",
    "    plt.plot(epochs, history.history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Loss\\nTest Loss: {test_loss:.4f}')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Experimenting with different hyperparameters\n",
    "def hyperparameter_analysis():\n",
    "    # Load datasets\n",
    "    mnist_data, fmnist_data = load_datasets()\n",
    "    \n",
    "    # Define hyperparameter ranges\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    dropout_rates = [0.1, 0.3, 0.5]\n",
    "    num_epochs = 50  # Use a small number of epochs for testing\n",
    "    \n",
    "    fig_count = 1  # Counter for figures\n",
    "    subplot_index = 1  # Counter for subplots in a row\n",
    "    \n",
    "    plt.figure(figsize=(30, 5))  # Create the figure to hold 8 subplots in a row\n",
    "\n",
    "    # Perform experiments for MNIST dataset\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for dropout_rate in dropout_rates:\n",
    "                param_info = f\"LR={lr}, BS={batch_size}, Drop={dropout_rate}\"\n",
    "                print(f\"Training with {param_info} on MNIST dataset\")\n",
    "                \n",
    "                # Train model with current hyperparameters\n",
    "                history, test_loss, test_acc = train_model(mnist_data, lr, batch_size, num_epochs, dropout_rate)\n",
    "                \n",
    "                # Plot in the corresponding subplot\n",
    "                plot_history(history, param_info, test_acc, test_loss, subplot_index)\n",
    "                \n",
    "                subplot_index += 1  # Move to the next subplot\n",
    "                \n",
    "                # Save and reset the figure when 8 subplots are filled\n",
    "                if subplot_index > 4:  # 4 accuracy subplots and 4 loss subplots make a total of 8\n",
    "                    plt.show()\n",
    "                    plt.figure(figsize=(30, 5))  # Create a new figure for the next set of 8 subplots\n",
    "                    subplot_index = 1  # Reset subplot index\n",
    "                    fig_count += 1\n",
    "\n",
    "    # Perform experiments for FashionMNIST dataset\n",
    "    fig_count = 1  # Reset counter for FashionMNIST\n",
    "    subplot_index = 1  # Reset subplot index\n",
    "    plt.figure(figsize=(30, 5))  # Create the figure to hold 8 subplots in a row\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            for dropout_rate in dropout_rates:\n",
    "                param_info = f\"LR={lr}, BS={batch_size}, Drop={dropout_rate}\"\n",
    "                print(f\"Training with {param_info} on Fashion-MNIST dataset\")\n",
    "                \n",
    "                # Train model with current hyperparameters\n",
    "                history, test_loss, test_acc = train_model(fmnist_data, lr, batch_size, num_epochs, dropout_rate)\n",
    "                \n",
    "                # Plot in the corresponding subplot\n",
    "                plot_history(history, param_info, test_acc, test_loss, subplot_index)\n",
    "                \n",
    "                subplot_index += 1  # Move to the next subplot\n",
    "                \n",
    "                # Save and reset the figure when 8 subplots are filled\n",
    "                if subplot_index > 4:  # 4 accuracy subplots and 4 loss subplots make a total of 8\n",
    "                    plt.show()\n",
    "                    plt.figure(figsize=(30, 5))  # Create a new figure for the next set of 8 subplots\n",
    "                    subplot_index = 1  # Reset subplot index\n",
    "                    fig_count += 1\n",
    "\n",
    "# Run hyperparameter analysis\n",
    "hyperparameter_analysis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745ee5a2-8ccf-4e15-a333-cd14e5165ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist, fashion_mnist\n",
    "\n",
    "# Hybrid Optimization Parameters\n",
    "switch_threshold = 0.01  # Threshold to switch from classical to quantum\n",
    "classical_learning_rate = 0.001\n",
    "quantum_learning_rate = 0.0001\n",
    "\n",
    "\n",
    "def quantum_gradient_update(variables, learning_rate, gradients):\n",
    "    # Apply simulated quantum optimization by simple parameter shift to each variable\n",
    "    for var, grad in zip(variables, gradients):\n",
    "        var.assign_sub(learning_rate * grad)\n",
    "\n",
    "# Classical Gradient Descent Update\n",
    "def classical_gradient_update(theta_c, learning_rate, gradients):\n",
    "    theta_c -= learning_rate * gradients\n",
    "    return theta_c\n",
    "\n",
    "# Build the neural network model\n",
    "def build_model(dropout_rate=0.2):\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train model with hybrid optimization\n",
    "def train_hybrid_model(dataset, learning_rate, batch_size, epochs, dropout_rate):\n",
    "    x_train, y_train, x_test, y_test = dataset\n",
    "    model = build_model(dropout_rate)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=classical_learning_rate)\n",
    "\n",
    "    history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "    # Perform training with hybrid optimization\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        # Use GradientTape to compute gradients\n",
    "        for step, (x_batch, y_batch) in enumerate(tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits = model(x_batch, training=True)\n",
    "                loss_value = tf.keras.losses.sparse_categorical_crossentropy(y_batch, logits)\n",
    "                loss_value = tf.reduce_mean(loss_value)\n",
    "\n",
    "            # Get gradients for the classical parameters (trainable variables)\n",
    "            gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "            norm_gradients = np.linalg.norm([tf.norm(grad).numpy() for grad in gradients])\n",
    "\n",
    "            # Switch to quantum update if classical gradient norm falls below threshold\n",
    "            if norm_gradients < switch_threshold:\n",
    "\n",
    "                quantum_gradient_update(model.trainable_variables, quantum_learning_rate, gradients)\n",
    "            else:\n",
    "                # Classical update\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            epoch_loss_avg.update_state(loss_value)\n",
    "            epoch_accuracy.update_state(y_batch, logits)\n",
    "\n",
    "        # Evaluate on test set after each epoch\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "        # Store training history\n",
    "        history['loss'].append(epoch_loss_avg.result().numpy())\n",
    "        history['accuracy'].append(epoch_accuracy.result().numpy())\n",
    "        history['val_loss'].append(test_loss)\n",
    "        history['val_accuracy'].append(test_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}: Loss = {epoch_loss_avg.result().numpy()}, Accuracy = {epoch_accuracy.result().numpy()}, Val Loss = {test_loss}, Val Accuracy = {test_acc}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# Function to load datasets\n",
    "def load_datasets():\n",
    "    (x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "    (x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()\n",
    "    \n",
    "    x_train_mnist, x_test_mnist = x_train_mnist / 255.0, x_test_mnist / 255.0\n",
    "    x_train_fmnist, x_test_fmnist = x_train_fmnist / 255.0, x_test_fmnist / 255.0\n",
    "    \n",
    "    return (x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist), (x_train_fmnist, y_train_fmnist, x_test_fmnist, y_test_fmnist)\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_results(history, title):\n",
    "    epochs = range(1, len(history['accuracy']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(epochs, history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{title} - Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history['loss'], label='Train Loss')\n",
    "    plt.plot(epochs, history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function to perform experiments\n",
    "def hybrid_optimization_experiment():\n",
    "    mnist_data, fmnist_data = load_datasets()\n",
    "    num_epochs = 20\n",
    "    batch_size = 64\n",
    "    dropout_rate = 0.2\n",
    "\n",
    "    # Train MNIST dataset\n",
    "    print(\"Training MNIST with hybrid approach...\")\n",
    "    history_mnist = train_hybrid_model(mnist_data, classical_learning_rate, batch_size, num_epochs, dropout_rate)\n",
    "    plot_results(history_mnist, \"MNIST Hybrid Optimization\")\n",
    "\n",
    "    # Train FashionMNIST dataset\n",
    "    print(\"Training Fashion-MNIST with hybrid approach...\")\n",
    "    history_fmnist = train_hybrid_model(fmnist_data, classical_learning_rate, batch_size, num_epochs, dropout_rate)\n",
    "    plot_results(history_fmnist, \"FashionMNIST Hybrid Optimization\")\n",
    "\n",
    "# Run the experiment\n",
    "hybrid_optimization_experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2c731-cb7e-4738-ade0-9f54237bce41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
